{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81825b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84dcb249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = CustomDataset()\n",
    "# val_dataset = CustomDataset()\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df8ec806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations to apply to the CIFAR-10 data\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "download = True\n",
    "# Define the training and test datasets\n",
    "train_dataset = datasets.Flowers102(root='./data', split=\"train\", download=download, transform=data_transforms)\n",
    "test_dataset = datasets.Flowers102(root='./data', split=\"test\", download=download, transform=data_transforms)\n",
    "\n",
    "# Define the dataloaders to load the data in batches during training and testing\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37650cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 16\n",
      "Number of testing batches: 97\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of testing batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "044e6a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network to test\n",
    "model = models.resnet18(pretrained=False)\n",
    "num_classes = 102\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Modify the last fully connected layer\n",
    "fc_input = model.fc.in_features\n",
    "model.fc = nn.Linear(fc_input, num_classes)\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# Step 5: Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4595ecdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 11228838\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(torch.numel(p) for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51effc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./logs/flowers\"  # Set the directory for storing the logs\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a3372db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cdeee83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss: 71.62561893463135 Accuracy: 3.4313725490196076\n",
      "Epoch 2 : Loss: 83.44756746292114 Accuracy: 2.450980392156863\n",
      "Epoch 3 : Loss: 72.786630153656 Accuracy: 2.156862745098039\n",
      "Epoch 4 : Loss: 70.43598651885986 Accuracy: 3.5294117647058822\n",
      "Epoch 5 : Loss: 67.73175287246704 Accuracy: 3.627450980392157\n",
      "Epoch 5 : Test Accuracy: 3.2850870060172386\n",
      "Epoch 6 : Loss: 65.91990351676941 Accuracy: 6.176470588235294\n",
      "Epoch 7 : Loss: 64.32340049743652 Accuracy: 6.862745098039215\n",
      "Epoch 8 : Loss: 63.408204793930054 Accuracy: 5.490196078431373\n",
      "Epoch 9 : Loss: 62.26842260360718 Accuracy: 9.019607843137255\n",
      "Epoch 10 : Loss: 61.68536925315857 Accuracy: 9.313725490196079\n",
      "Epoch 10 : Test Accuracy: 7.350788746137583\n",
      "Epoch 11 : Loss: 61.155158281326294 Accuracy: 8.235294117647058\n",
      "Epoch 12 : Loss: 59.005356550216675 Accuracy: 11.176470588235293\n",
      "Epoch 13 : Loss: 58.17576336860657 Accuracy: 11.372549019607844\n",
      "Epoch 14 : Loss: 56.42585253715515 Accuracy: 15.882352941176471\n",
      "Epoch 15 : Loss: 54.65356516838074 Accuracy: 18.333333333333332\n",
      "Epoch 15 : Test Accuracy: 9.676370141486421\n",
      "Epoch 16 : Loss: 53.28885316848755 Accuracy: 18.725490196078432\n",
      "Epoch 17 : Loss: 51.737754583358765 Accuracy: 20.88235294117647\n",
      "Epoch 18 : Loss: 49.40994191169739 Accuracy: 24.313725490196077\n",
      "Epoch 19 : Loss: 49.1674599647522 Accuracy: 23.92156862745098\n",
      "Epoch 20 : Loss: 47.755231857299805 Accuracy: 25.784313725490197\n",
      "Epoch 20 : Test Accuracy: 10.73345259391771\n",
      "Epoch 21 : Loss: 45.4636116027832 Accuracy: 34.21568627450981\n",
      "Epoch 22 : Loss: 42.623419523239136 Accuracy: 35.0\n",
      "Epoch 23 : Loss: 40.04552412033081 Accuracy: 39.705882352941174\n",
      "Epoch 24 : Loss: 36.33190178871155 Accuracy: 38.92156862745098\n",
      "Epoch 25 : Loss: 34.748047828674316 Accuracy: 44.6078431372549\n",
      "Epoch 25 : Test Accuracy: 14.083590827776874\n",
      "Epoch 26 : Loss: 30.88275671005249 Accuracy: 60.588235294117645\n",
      "Epoch 27 : Loss: 26.179916501045227 Accuracy: 54.90196078431372\n",
      "Epoch 28 : Loss: 24.2855623960495 Accuracy: 70.58823529411765\n",
      "Epoch 29 : Loss: 19.93772256374359 Accuracy: 65.29411764705883\n",
      "Epoch 30 : Loss: 18.2145916223526 Accuracy: 69.90196078431373\n",
      "Epoch 30 : Test Accuracy: 14.408846966986502\n",
      "Epoch 31 : Loss: 16.823392152786255 Accuracy: 81.56862745098039\n",
      "Epoch 32 : Loss: 11.627642631530762 Accuracy: 83.92156862745098\n",
      "Epoch 33 : Loss: 10.286398202180862 Accuracy: 85.0\n",
      "Epoch 34 : Loss: 9.189176827669144 Accuracy: 86.07843137254902\n",
      "Epoch 35 : Loss: 7.3011588752269745 Accuracy: 89.31372549019608\n",
      "Epoch 35 : Test Accuracy: 14.083590827776874\n",
      "Epoch 36 : Loss: 6.111528262495995 Accuracy: 93.03921568627452\n",
      "Epoch 37 : Loss: 4.926408961415291 Accuracy: 95.29411764705883\n",
      "Epoch 38 : Loss: 4.237553492188454 Accuracy: 95.68627450980392\n",
      "Epoch 39 : Loss: 4.447757832705975 Accuracy: 95.58823529411765\n",
      "Epoch 40 : Loss: 3.3827749267220497 Accuracy: 93.62745098039215\n",
      "Epoch 40 : Test Accuracy: 14.929256789721906\n",
      "Epoch 41 : Loss: 6.002361848950386 Accuracy: 95.7843137254902\n",
      "Epoch 42 : Loss: 3.0142007172107697 Accuracy: 97.45098039215686\n",
      "Epoch 43 : Loss: 2.2814993113279343 Accuracy: 99.50980392156863\n",
      "Epoch 44 : Loss: 1.4134468883275986 Accuracy: 97.3529411764706\n",
      "Epoch 45 : Loss: 1.3772264067083597 Accuracy: 97.84313725490196\n",
      "Epoch 45 : Test Accuracy: 16.083916083916083\n",
      "Epoch 46 : Loss: 0.9926024656742811 Accuracy: 98.62745098039215\n",
      "Epoch 47 : Loss: 1.514123905915767 Accuracy: 98.72549019607843\n",
      "Epoch 48 : Loss: 0.8286882298998535 Accuracy: 99.31372549019608\n",
      "Epoch 49 : Loss: 0.5200018477626145 Accuracy: 99.50980392156863\n",
      "Epoch 50 : Loss: 0.5934858722612262 Accuracy: 99.70588235294117\n",
      "Epoch 50 : Test Accuracy: 17.44999186859652\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      9\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     12\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/private/.env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/private/.env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/private/.env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/private/.env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "log_train_every = 1\n",
    "log_test_every = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "#         if (i+1) % 100 == 0:\n",
    "#             print(f\"[Epoch: {epoch + 1}, Batch: {i + 1}] Loss: {running_loss/100:.3f}\")\n",
    "#             running_loss = 0.0\n",
    "        \n",
    "    # compute training & testing accuracy every couple of iterations        \n",
    "    if (epoch+1) % log_train_every == 0:\n",
    "        train_accuracy = model_eval(model, train_loader)\n",
    "\n",
    "        # Log the loss\n",
    "        #writer.add_scalar('Loss/train', loss.cpu().item(), epoch * len(train_loader) + i)\n",
    "\n",
    "        # Log the training accuracy\n",
    "        #writer.add_scalar('Accuracy/train', train_accuracy, epoch * len(train_loader) + i)\n",
    "        print(f\"Epoch {epoch+1} : Loss: {running_loss} Accuracy: {train_accuracy}\")\n",
    "        \n",
    "    if (epoch+1) % log_test_every == 0:\n",
    "        test_accuracy = model_eval(model, test_loader)\n",
    "\n",
    "        # Log the test accuracy\n",
    "        #writer.add_scalar('Accuracy/test', test_accuracy, epoch * len(train_loader) + i)\n",
    "        print(f\"Epoch {epoch+1} : Test Accuracy: {test_accuracy}\")\n",
    "        \n",
    "writer.close()\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786426e5",
   "metadata": {},
   "source": [
    "## Curriculum Learning experiment 1\n",
    "\n",
    "So.. The curriculum for the following training procedure is that we increase the number of classes progressively in the dataset. All original labels are shited by 1. So, label 0 becomes 1, 1 becomes 2.. and so on. \n",
    "We first start with 2 classes 0 and 1 where 0 represents rest of classes and 1 represents the class that was originally 0. Then we work with 3 classes after 2 epochs: 0(rest of the classes), 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40b38146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubsetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, classes, num_classes, train=True, total_classes=102):\n",
    "        self.dataset = dataset\n",
    "        self.classes = classes\n",
    "        self.num_classes = num_classes\n",
    "        self.train = train\n",
    "        self.total_classes = total_classes\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.dataset[index]\n",
    "        if not self.train:\n",
    "            return image, (label+1)%self.total_classes# we would reserve 0 for non-existing classes\n",
    "        \n",
    "        if label in self.classes[:self.num_classes]:\n",
    "            return image, (label+1)%self.total_classes\n",
    "        else:\n",
    "            return torch.zeros_like(image), 0# IMPORTANT CHOICE MADE HERE \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd6baa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss: 30.097966594155878 Accuracy: 99.01960784313725\n",
      "Epoch 2 : Loss: 5.318315498530865 Accuracy: 96.96078431372548\n",
      "Epoch 3 : Loss: 2.7389416117221117 Accuracy: 95.09803921568627\n",
      "Epoch 4 : Loss: 3.874854639172554 Accuracy: 93.13725490196079\n",
      "Epoch 5 : Loss: 4.6901357769966125 Accuracy: 91.17647058823529\n",
      "Epoch 5 : Test Accuracy: 0.9803921568627451\n",
      "Epoch 6 : Loss: 5.725368015468121 Accuracy: 90.0\n",
      "Epoch 7 : Loss: 6.797103315591812 Accuracy: 88.33333333333333\n",
      "Epoch 8 : Loss: 7.750844806432724 Accuracy: 85.98039215686275\n",
      "Epoch 9 : Loss: 9.018923714756966 Accuracy: 84.2156862745098\n",
      "Epoch 10 : Loss: 9.751175552606583 Accuracy: 81.96078431372548\n",
      "Epoch 10 : Test Accuracy: 1.5686274509803921\n",
      "Epoch 11 : Loss: 11.22835624217987 Accuracy: 80.68627450980392\n",
      "Epoch 12 : Loss: 11.773661375045776 Accuracy: 79.01960784313725\n",
      "Epoch 13 : Loss: 12.610673189163208 Accuracy: 78.13725490196079\n",
      "Epoch 14 : Loss: 14.453419268131256 Accuracy: 75.0\n",
      "Epoch 15 : Loss: 15.424874186515808 Accuracy: 74.2156862745098\n",
      "Epoch 15 : Test Accuracy: 3.627450980392157\n",
      "Epoch 16 : Loss: 16.292681753635406 Accuracy: 72.94117647058823\n",
      "Epoch 17 : Loss: 17.203051328659058 Accuracy: 70.58823529411765\n",
      "Epoch 18 : Loss: 18.49187672138214 Accuracy: 69.41176470588235\n",
      "Epoch 19 : Loss: 19.509059190750122 Accuracy: 68.52941176470588\n",
      "Epoch 20 : Loss: 20.729200661182404 Accuracy: 65.98039215686275\n",
      "Epoch 20 : Test Accuracy: 5.196078431372549\n",
      "Epoch 21 : Loss: 22.77021038532257 Accuracy: 64.11764705882354\n",
      "Epoch 22 : Loss: 23.612035632133484 Accuracy: 63.627450980392155\n",
      "Epoch 23 : Loss: 24.649850964546204 Accuracy: 62.84313725490196\n",
      "Epoch 24 : Loss: 25.142494440078735 Accuracy: 60.490196078431374\n",
      "Epoch 25 : Loss: 25.047144889831543 Accuracy: 60.78431372549019\n",
      "Epoch 25 : Test Accuracy: 9.803921568627452\n",
      "Epoch 26 : Loss: 25.704004645347595 Accuracy: 59.80392156862745\n",
      "Epoch 27 : Loss: 26.096425652503967 Accuracy: 60.98039215686274\n",
      "Epoch 28 : Loss: 27.26653289794922 Accuracy: 57.254901960784316\n",
      "Epoch 29 : Loss: 26.414132952690125 Accuracy: 55.490196078431374\n",
      "Epoch 30 : Loss: 84.420978307724 Accuracy: 53.333333333333336\n",
      "Epoch 30 : Test Accuracy: 12.254901960784315\n",
      "Epoch 31 : Loss: 31.85455310344696 Accuracy: 52.94117647058823\n",
      "Epoch 32 : Loss: 30.717549324035645 Accuracy: 52.254901960784316\n",
      "Epoch 33 : Loss: 31.32061517238617 Accuracy: 55.0\n",
      "Epoch 34 : Loss: 30.09907853603363 Accuracy: 53.529411764705884\n",
      "Epoch 35 : Loss: 28.53178369998932 Accuracy: 56.76470588235294\n",
      "Epoch 35 : Test Accuracy: 25.392156862745097\n",
      "Epoch 36 : Loss: 28.882667779922485 Accuracy: 57.84313725490196\n",
      "Epoch 37 : Loss: 28.58236289024353 Accuracy: 60.0\n",
      "Epoch 38 : Loss: 28.34679865837097 Accuracy: 56.666666666666664\n",
      "Epoch 39 : Loss: 27.555566787719727 Accuracy: 60.294117647058826\n",
      "Epoch 40 : Loss: 25.599263429641724 Accuracy: 63.72549019607843\n",
      "Epoch 40 : Test Accuracy: 42.15686274509804\n",
      "Epoch 41 : Loss: 26.281729578971863 Accuracy: 64.70588235294117\n",
      "Epoch 42 : Loss: 25.336147904396057 Accuracy: 68.43137254901961\n",
      "Epoch 43 : Loss: 23.80798041820526 Accuracy: 66.56862745098039\n",
      "Epoch 44 : Loss: 22.22694754600525 Accuracy: 71.86274509803921\n",
      "Epoch 45 : Loss: 21.61219757795334 Accuracy: 75.3921568627451\n",
      "Epoch 45 : Test Accuracy: 63.627450980392155\n",
      "Epoch 46 : Loss: 21.724327445030212 Accuracy: 72.3529411764706\n",
      "Epoch 47 : Loss: 21.05265301465988 Accuracy: 74.01960784313725\n",
      "Epoch 48 : Loss: 19.12540465593338 Accuracy: 80.49019607843137\n",
      "Epoch 49 : Loss: 17.662562429904938 Accuracy: 84.80392156862744\n",
      "Epoch 50 : Loss: 14.959592342376709 Accuracy: 84.50980392156863\n",
      "Epoch 50 : Test Accuracy: 82.54901960784314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-572:\n",
      "Exception ignored in: <function _releaseLock at 0x7fda3b5a43a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/logging/__init__.py\", line 227, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/asagrawal/private/.env/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 208, in _worker_loop\n",
      "    def _worker_loop(dataset_kind, dataset, index_queue, data_queue, done_event,\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 58948, 59011) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/private/.env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m~/private/.env/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 59011) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Create the train loader using the modified dataset\u001b[39;00m\n\u001b[1;32m     25\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(subset_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     29\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     31\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/private/.env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/private/.env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/private/.env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/private/.env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1145\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1144\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 58948, 59011) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "curriculum_epochs = 5\n",
    "increment_step = 2\n",
    "nclass = increment_step\n",
    "\n",
    "\n",
    "log_train_every = 1\n",
    "log_test_every = 5\n",
    "\n",
    "test_dataset = SubsetDataset(train_dataset, list(range(num_classes)), num_classes, train=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    subset_classes = list(range(nclass))\n",
    "\n",
    "    # Create the modified dataset\n",
    "    subset_dataset = SubsetDataset(train_dataset, subset_classes, nclass)\n",
    "\n",
    "    # Create the train loader using the modified dataset\n",
    "    train_loader = torch.utils.data.DataLoader(subset_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "#         if (i+1) % 100 == 0:\n",
    "#             print(f\"[Epoch: {epoch + 1}, Batch: {i + 1}] Loss: {running_loss/100:.3f}\")\n",
    "#             running_loss = 0.0\n",
    "        \n",
    "    # compute training & testing accuracy every couple of iterations        \n",
    "    if (epoch+1) % log_train_every == 0:\n",
    "        train_accuracy = model_eval(model, train_loader)\n",
    "\n",
    "        # Log the loss\n",
    "        #writer.add_scalar('Loss/train', loss.cpu().item(), epoch * len(train_loader) + i)\n",
    "\n",
    "        # Log the training accuracy\n",
    "        #writer.add_scalar('Accuracy/train', train_accuracy, epoch * len(train_loader) + i)\n",
    "        print(f\"Epoch {epoch+1} : Loss: {running_loss} Accuracy: {train_accuracy}\")\n",
    "        \n",
    "    if (epoch+1) % log_test_every == 0:\n",
    "        test_accuracy = model_eval(model, test_loader)\n",
    "\n",
    "        # Log the test accuracy\n",
    "        #writer.add_scalar('Accuracy/test', test_accuracy, epoch * len(train_loader) + i)\n",
    "        print(f\"Epoch {epoch+1} : Test Accuracy: {test_accuracy}\")\n",
    "    \n",
    "    nclass += increment_step\n",
    "\n",
    "writer.close()\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393a0d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e299c20b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 2.0",
   "language": "python",
   "name": "torch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
