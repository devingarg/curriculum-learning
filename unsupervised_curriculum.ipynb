{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81825b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils.data import get_data_loaders\n",
    "from utils.train_eval import train, train_curriculum\n",
    "from utils.misc import get_features, cluster_features\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "FEAT_DIM=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8ec806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Either cifar10 or flowers102\n",
    "dataset = \"cifar10\"\n",
    "data_config = {\n",
    "    \"batch_size\": 64,\n",
    "    \"num_workers\": 2,\n",
    "}\n",
    "loaders, num_classes, datasets = get_data_loaders(dataset=dataset,\n",
    "                                                  batch_size=data_config[\"batch_size\"],\n",
    "                                                  num_workers=data_config[\"num_workers\"],\n",
    "                                                  return_dataset=True)\n",
    "\n",
    "train_loader, test_loader = loaders\n",
    "train_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37650cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 782\n",
      "Number of testing batches: 157\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of testing batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a00abed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devin/envs/vision/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/devin/envs/vision/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# which model to use for feature extraction?\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "extractor = vgg16.features\n",
    "\n",
    "# features for all the samples in the train dataloader\n",
    "feats = get_features(extractor, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8bacfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE: If there are too many samples, training KMeans can take a \n",
    "lot of time. To prevent that from happening, you can limit the number\n",
    "of samples being used to train the KMeans using this cell. \n",
    "E.g., uncomment the last line to only use the first 1000 features.\n",
    "\"\"\"\n",
    "features = feats\n",
    "# features = feats[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3052c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dists = get_pairwise_distance(feats)\n",
    "# print(dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69632efd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Number of clusters to split the input samples into using KMeans \u001b[39;00m\n\u001b[1;32m      2\u001b[0m num_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 3\u001b[0m c_labels \u001b[38;5;241m=\u001b[39m \u001b[43mcluster_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_clusters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cleaner/curriculum-learning/utils/misc.py:42\u001b[0m, in \u001b[0;36mcluster_features\u001b[0;34m(features, num_clusters)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcluster_features\u001b[39m(features, num_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    Given N features & a cluster count as input, clusters the features\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    using KMeans and returns an array containing N cluster assignments - \u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    one for every input feature.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     cobj \u001b[38;5;241m=\u001b[39m \u001b[43mKMeans\u001b[49m(n_clusters\u001b[38;5;241m=\u001b[39mnum_clusters)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# do the clustering\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     cobj\u001b[38;5;241m.\u001b[39mfit(features)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KMeans' is not defined"
     ]
    }
   ],
   "source": [
    "# Number of clusters to split the input samples into using KMeans \n",
    "num_clusters = 5\n",
    "c_labels = cluster_features(features, num_clusters=num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23ca3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out the data into clusters\n",
    "from collections import defaultdict\n",
    "\n",
    "clustered_data = defaultdict(list)\n",
    "\n",
    "for idx, l in enumerate(c_labels):\n",
    "    clustered_data[l].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab25b132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 17686), (0, 11627), (3, 11611), (4, 4729), (2, 4347)]\n"
     ]
    }
   ],
   "source": [
    "# How big are the different clusters?\n",
    "c_size = []\n",
    "\n",
    "for l in clustered_data.keys():\n",
    "    c_size.append((l, len(clustered_data[l])))\n",
    "\n",
    "# sort by the number of samples in the cluster\n",
    "c_size = sorted(c_size, key=lambda x: x[1], reverse=True)\n",
    "print(c_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44a10547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network to be trained\n",
    "model = models.resnet18(pretrained=False)\n",
    "\n",
    "# Change the output of the last FC layer as per the number of classes\n",
    "fc_input = model.fc.in_features\n",
    "model.fc = nn.Linear(fc_input, num_classes)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1175a73d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1 (35.37% data) done. Test Acc: 56.960\n",
      "Cluster 0 (23.25% data) done. Test Acc: 53.510\n",
      "Cluster 3 (23.22% data) done. Test Acc: 54.800\n",
      "Cluster 4 (9.46% data) done. Test Acc: 49.780\n",
      "Cluster 2 (8.69% data) done. Test Acc: 38.860\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "# How many epochs to train for (per cluster)?\n",
    "num_epochs = 10\n",
    "\n",
    "\"\"\" We experimented with two different ways to present data to the model:\n",
    "1. Present clusters in increasing order of cluster size (mode: S2L)\n",
    "2. Present clusters in decreasing order of cluster size (mode: L2S)\n",
    "\"\"\"\n",
    "mode = \"L2S\"\n",
    "\n",
    "train_curriculum(model, dataset, train_dataset, clustered_data, c_idx, mode, num_epochs, data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "462da800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual training: Fine-tune the model on the entire dataset for a few epochs\n",
    "\n",
    "num_epochs_res = 2\n",
    "\n",
    "# TensorBoard log directory\n",
    "log_dir = f\"./logs/{dataset}_vgg16_{mode}_{num_epochs}_c{num_clusters}_residual{num_epochs_res}\"\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "config = {\n",
    "    \"opt\": optimizer,\n",
    "    \"crit\": criterion,\n",
    "    \"log_freq_test\": 250,\n",
    "    \"log_freq_tr\": 150, \n",
    "}\n",
    "\n",
    "train(model, train_loader, test_loader, num_epochs_res, config, device, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c73a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
