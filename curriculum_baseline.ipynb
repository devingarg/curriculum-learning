{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81825b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede930ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         pass\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84dcb249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = CustomDataset()\n",
    "# val_dataset = CustomDataset()\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df8ec806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define the transformations to apply to the CIFAR-10 data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the image tensors\n",
    "])\n",
    "\n",
    "# Define the training and test datasets\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define the dataloaders to load the data in batches during training and testing\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37650cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 782\n",
      "Number of testing batches: 157\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of testing batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "044e6a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network to test\n",
    "model = models.resnet18(pretrained=False)\n",
    "num_classes = 10\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Modify the last fully connected layer\n",
    "fc_input = model.fc.in_features\n",
    "model.fc = nn.Linear(fc_input, num_classes)\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# Step 5: Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4595ecdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 11181642\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(torch.numel(p) for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51effc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 21:10:24.709537: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-29 21:10:25.926168: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "log_dir = \"./logs/scratch\"  # Set the directory for storing the logs\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a3372db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cdeee83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1, Batch: 100] Loss: 1.812\n",
      "[Epoch: 1, Batch: 200] Loss: 1.634\n",
      "[Epoch: 1, Batch: 300] Loss: 1.530\n",
      "[Epoch: 1, Batch: 400] Loss: 1.411\n",
      "[Epoch: 1, Batch: 500] Loss: 1.338\n",
      "[Epoch: 1, Batch: 600] Loss: 1.275\n",
      "[Epoch: 1, Batch: 700] Loss: 1.268\n",
      "[Epoch: 2, Batch: 100] Loss: 1.168\n",
      "[Epoch: 2, Batch: 200] Loss: 1.136\n",
      "[Epoch: 2, Batch: 300] Loss: 1.096\n",
      "[Epoch: 2, Batch: 400] Loss: 1.095\n",
      "[Epoch: 2, Batch: 500] Loss: 1.048\n",
      "[Epoch: 2, Batch: 600] Loss: 1.050\n",
      "[Epoch: 2, Batch: 700] Loss: 1.015\n",
      "[Epoch: 3, Batch: 100] Loss: 0.911\n",
      "[Epoch: 3, Batch: 200] Loss: 0.897\n",
      "[Epoch: 3, Batch: 300] Loss: 0.900\n",
      "[Epoch: 3, Batch: 400] Loss: 0.903\n",
      "[Epoch: 3, Batch: 500] Loss: 0.883\n",
      "[Epoch: 3, Batch: 600] Loss: 0.863\n",
      "[Epoch: 3, Batch: 700] Loss: 0.871\n",
      "[Epoch: 4, Batch: 100] Loss: 0.704\n",
      "[Epoch: 4, Batch: 200] Loss: 0.761\n",
      "[Epoch: 4, Batch: 300] Loss: 0.755\n",
      "[Epoch: 4, Batch: 400] Loss: 0.736\n",
      "[Epoch: 4, Batch: 500] Loss: 0.755\n",
      "[Epoch: 4, Batch: 600] Loss: 0.731\n",
      "[Epoch: 4, Batch: 700] Loss: 0.786\n",
      "[Epoch: 5, Batch: 100] Loss: 0.595\n",
      "[Epoch: 5, Batch: 200] Loss: 0.613\n",
      "[Epoch: 5, Batch: 300] Loss: 0.621\n",
      "[Epoch: 5, Batch: 400] Loss: 0.619\n",
      "[Epoch: 5, Batch: 500] Loss: 0.648\n",
      "[Epoch: 5, Batch: 600] Loss: 0.663\n",
      "[Epoch: 5, Batch: 700] Loss: 0.669\n",
      "[Epoch: 6, Batch: 100] Loss: 0.455\n",
      "[Epoch: 6, Batch: 200] Loss: 0.483\n",
      "[Epoch: 6, Batch: 300] Loss: 0.506\n",
      "[Epoch: 6, Batch: 400] Loss: 0.524\n",
      "[Epoch: 6, Batch: 500] Loss: 0.536\n",
      "[Epoch: 6, Batch: 600] Loss: 0.541\n",
      "[Epoch: 6, Batch: 700] Loss: 0.549\n",
      "[Epoch: 7, Batch: 100] Loss: 0.378\n",
      "[Epoch: 7, Batch: 200] Loss: 0.390\n",
      "[Epoch: 7, Batch: 300] Loss: 0.428\n",
      "[Epoch: 7, Batch: 400] Loss: 0.444\n",
      "[Epoch: 7, Batch: 500] Loss: 0.448\n",
      "[Epoch: 7, Batch: 600] Loss: 0.448\n",
      "[Epoch: 7, Batch: 700] Loss: 0.438\n",
      "[Epoch: 8, Batch: 100] Loss: 0.284\n",
      "[Epoch: 8, Batch: 200] Loss: 0.315\n",
      "[Epoch: 8, Batch: 300] Loss: 0.353\n",
      "[Epoch: 8, Batch: 400] Loss: 0.363\n",
      "[Epoch: 8, Batch: 500] Loss: 0.387\n",
      "[Epoch: 8, Batch: 600] Loss: 0.373\n",
      "[Epoch: 8, Batch: 700] Loss: 0.362\n",
      "[Epoch: 9, Batch: 100] Loss: 0.231\n",
      "[Epoch: 9, Batch: 200] Loss: 0.266\n",
      "[Epoch: 9, Batch: 300] Loss: 0.275\n",
      "[Epoch: 9, Batch: 400] Loss: 0.286\n",
      "[Epoch: 9, Batch: 500] Loss: 0.290\n",
      "[Epoch: 9, Batch: 600] Loss: 0.299\n",
      "[Epoch: 9, Batch: 700] Loss: 0.331\n",
      "[Epoch: 10, Batch: 100] Loss: 0.167\n",
      "[Epoch: 10, Batch: 200] Loss: 0.213\n",
      "[Epoch: 10, Batch: 300] Loss: 0.212\n",
      "[Epoch: 10, Batch: 400] Loss: 0.238\n",
      "[Epoch: 10, Batch: 500] Loss: 0.272\n",
      "[Epoch: 10, Batch: 600] Loss: 0.258\n",
      "[Epoch: 10, Batch: 700] Loss: 0.261\n",
      "[Epoch: 11, Batch: 100] Loss: 0.147\n",
      "[Epoch: 11, Batch: 200] Loss: 0.164\n",
      "[Epoch: 11, Batch: 300] Loss: 0.195\n",
      "[Epoch: 11, Batch: 400] Loss: 0.198\n",
      "[Epoch: 11, Batch: 500] Loss: 0.221\n",
      "[Epoch: 11, Batch: 600] Loss: 0.224\n",
      "[Epoch: 11, Batch: 700] Loss: 0.236\n",
      "[Epoch: 12, Batch: 100] Loss: 0.130\n",
      "[Epoch: 12, Batch: 200] Loss: 0.138\n",
      "[Epoch: 12, Batch: 300] Loss: 0.161\n",
      "[Epoch: 12, Batch: 400] Loss: 0.185\n",
      "[Epoch: 12, Batch: 500] Loss: 0.190\n",
      "[Epoch: 12, Batch: 600] Loss: 0.189\n",
      "[Epoch: 12, Batch: 700] Loss: 0.212\n",
      "[Epoch: 13, Batch: 100] Loss: 0.142\n",
      "[Epoch: 13, Batch: 200] Loss: 0.125\n",
      "[Epoch: 13, Batch: 300] Loss: 0.153\n",
      "[Epoch: 13, Batch: 400] Loss: 0.149\n",
      "[Epoch: 13, Batch: 500] Loss: 0.187\n",
      "[Epoch: 13, Batch: 600] Loss: 0.174\n",
      "[Epoch: 13, Batch: 700] Loss: 0.180\n",
      "[Epoch: 14, Batch: 100] Loss: 0.087\n",
      "[Epoch: 14, Batch: 200] Loss: 0.117\n",
      "[Epoch: 14, Batch: 300] Loss: 0.140\n",
      "[Epoch: 14, Batch: 400] Loss: 0.143\n",
      "[Epoch: 14, Batch: 500] Loss: 0.154\n",
      "[Epoch: 14, Batch: 600] Loss: 0.166\n",
      "[Epoch: 14, Batch: 700] Loss: 0.167\n",
      "[Epoch: 15, Batch: 100] Loss: 0.093\n",
      "[Epoch: 15, Batch: 200] Loss: 0.119\n",
      "[Epoch: 15, Batch: 300] Loss: 0.115\n",
      "[Epoch: 15, Batch: 400] Loss: 0.139\n",
      "[Epoch: 15, Batch: 500] Loss: 0.135\n",
      "[Epoch: 15, Batch: 600] Loss: 0.145\n",
      "[Epoch: 15, Batch: 700] Loss: 0.152\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "log_train_every = 150\n",
    "log_test_every = 250\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"[Epoch: {epoch + 1}, Batch: {i + 1}] Loss: {running_loss/100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        # compute training & testing accuracy every couple of iterations        \n",
    "        if (i+1) % log_train_every == 0:\n",
    "            train_accuracy = model_eval(model, train_loader)\n",
    "            \n",
    "            # Log the loss\n",
    "            writer.add_scalar('Loss/train', loss.cpu().item(), epoch * len(train_loader) + i)\n",
    "        \n",
    "            # Log the training accuracy\n",
    "            writer.add_scalar('Accuracy/train', train_accuracy, epoch * len(train_loader) + i)\n",
    "        \n",
    "        if (i+1) % log_test_every == 0:\n",
    "            test_accuracy = model_eval(model, test_loader)\n",
    "            \n",
    "            # Log the test accuracy\n",
    "            writer.add_scalar('Accuracy/test', test_accuracy, epoch * len(train_loader) + i)\n",
    "        \n",
    "writer.close()\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462da800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
