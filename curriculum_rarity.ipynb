{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81825b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "FEAT_DIM=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8ec806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define the transformations to apply to the CIFAR-10 data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the image tensors\n",
    "])\n",
    "\n",
    "# Define the training and test datasets\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define the dataloaders to load the data in batches during training and testing\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37650cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 782\n",
      "Number of testing batches: 157\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of testing batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81f0a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract vgg features\n",
    "def get_features(extractor, dataloader):\n",
    "    \n",
    "    extractor.eval()\n",
    "    extractor = extractor.to(device)\n",
    "    result = None\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        output = extractor(x.to(device)).squeeze().detach().cpu().numpy()\n",
    "        if type(result)==np.ndarray:\n",
    "            # not empty\n",
    "            result = np.concatenate([result, output], axis=0)\n",
    "        else:\n",
    "            result = output.copy()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a00abed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)\n",
    "extractor = vgg16.features\n",
    "\n",
    "feats = get_features(extractor, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a8bacfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with the first 1000 features\n",
    "# features = feats[:1000]\n",
    "features = feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3052c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pairwise distance\n",
    "def get_pairwise_distance(features):\n",
    "    \n",
    "    dist = pdist(features, metric=\"euclidean\")\n",
    "    return dist\n",
    "\n",
    "# dists = get_pairwise_distance(feats)\n",
    "# print(dists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "159727f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster the features\n",
    "def cluster_features(features, num_clusters=5):\n",
    "    \n",
    "    cobj = KMeans(n_clusters=num_clusters)\n",
    "    \n",
    "    # do the clustering\n",
    "    cobj.fit(features)\n",
    "    \n",
    "    # get cluster assignments for the feature vectors\n",
    "    assignments = cobj.labels_\n",
    "    \n",
    "    return assignments    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69632efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: experiments to see what number of cluster assignments works well \n",
    "NUM_CLUSTERS=5\n",
    "c_labels = cluster_features(features, num_clusters=NUM_CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d23ca3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out the data into clusters\n",
    "from collections import defaultdict\n",
    "\n",
    "clustered_data = defaultdict(list)\n",
    "\n",
    "for idx, l in enumerate(c_labels):\n",
    "    clustered_data[l].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab25b132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 17686), (0, 11627), (3, 11611), (4, 4729), (2, 4347)]\n"
     ]
    }
   ],
   "source": [
    "# cluster indices and number of samples per cluster\n",
    "idx_size = []\n",
    "\n",
    "for l in clustered_data.keys():\n",
    "    idx_size.append((l, len(clustered_data[l])))\n",
    "\n",
    "# sort by the number of samples in the cluster\n",
    "idx_size = sorted(idx_size, key=lambda x: x[1], reverse=True)\n",
    "print(idx_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a3372db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            \n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44a10547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network to be trained\n",
    "model = models.resnet18(pretrained=False)\n",
    "num_classes = 10\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Modify the last fully connected layer\n",
    "fc_input = model.fc.in_features\n",
    "model.fc = nn.Linear(fc_input, num_classes)\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# Step 5: Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1175a73d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1 (35.37% data) done. Test Acc: 56.960\n",
      "Cluster 0 (23.25% data) done. Test Acc: 53.510\n",
      "Cluster 3 (23.22% data) done. Test Acc: 54.800\n",
      "Cluster 4 (9.46% data) done. Test Acc: 49.780\n",
      "Cluster 2 (8.69% data) done. Test Acc: 38.860\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "NUM_EPOCHS=10\n",
    "MODE=\"L2S\"\n",
    "\n",
    "# train sequentially on different clusters (larger to smaller)\n",
    "for c_idx, _ in idx_size:\n",
    "    \n",
    "    log_dir = f\"./logs/vgg16_{MODE}_EQ{NUM_EPOCHS}_c{NUM_CLUSTERS}_{c_idx}\"  # Set the directory for storing the logs\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    \n",
    "    # record the indices\n",
    "    idx_size_str = ', '.join(str(val) for val in idx_size)\n",
    "    writer.add_text(\"cluster sizes\", idx_size_str)\n",
    "    \n",
    "    # create dataloader for the cluster\n",
    "    cluster_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                                 batch_size=8,\n",
    "                                                 num_workers=2, \n",
    "                                                 sampler=SubsetRandomSampler(clustered_data[c_idx]),\n",
    "                                                 drop_last=True)\n",
    "    \n",
    "    # train using this data-loader\n",
    "    for epoch in range(NUM_EPOCHS):         # TBD: How many epochs per cluster?\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(cluster_loader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            # zero out the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get predictions and compute loss\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # track the loss\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Log the train loss\n",
    "        epoch_loss = running_loss/len(cluster_loader)\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch+1)\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Log the test accuracy\n",
    "        test_accuracy = model_eval(model, test_loader)\n",
    "        writer.add_scalar('Accuracy/test', test_accuracy, epoch+1)\n",
    "    \n",
    "    writer.close()\n",
    "    print(f\"Cluster {c_idx} ({len(clustered_data[c_idx])/len(train_dataset)*100:.2f}% data) done. Test Acc: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "462da800",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c73a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
